{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247808"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "rows=[]\n",
    "directory = os.path.join('/home/deepak/StackOverflow Data')\n",
    "for root,dirs,files in os.walk(directory):\n",
    "    for file in files:\n",
    "       if file.endswith(\".csv\"):\n",
    "           f=open(directory + \"/\" + file, 'rt')\n",
    "           reader = csv.reader(f)\n",
    "           for row in reader:\n",
    "                rows.append(row)\n",
    "       f.close()\n",
    "\n",
    "len(rows)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Done\n"
     ]
    }
   ],
   "source": [
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('\\n', ' ')\n",
    "    norm_text = re.sub(\"<code>([^>]*)</code>\" , \" \",norm_text)\n",
    "    norm_text = re.sub(\"<*\\w+>\", \" \",norm_text)\n",
    "    norm_text = re.sub(\"([^[a-zA-Z])([^\\w+\\.\\w+\\.\\w+])\",\" \",norm_text)\n",
    "    norm_text = re.sub(\"\\s+\\w{1}\\s+\",\" \",norm_text)\n",
    "    norm_text = norm_text.replace('...', ' ')\n",
    "    return re.split(\"\\s+\",norm_text)\n",
    "\n",
    "# normalize_text(rows[2][2])\n",
    "\n",
    "cleaned_docs=[normalize_text(row[1]+ row[2]) for row in rows ]\n",
    "\n",
    "print(\"Cleaning Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247808 docs:\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags')\n",
    "\n",
    "alldocs = []  # will hold all docs in original order\n",
    "\n",
    "for doc_no, doc in enumerate(cleaned_docs):\n",
    "    words=doc[1:]\n",
    "    tags=[doc_no]\n",
    "    alldocs.append(SentimentDocument(words,tags))\n",
    "print('%d docs:' % (len(alldocs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"this will be painfully slow otherwise\"\n",
    "\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "#     Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "#     # PV-DBOW \n",
    "#     Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "#     # PV-DM w/average\n",
    "        Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM/concat requires one special NULL word so it serves as template\n",
    "\n",
    "print(simple_models[0])\n",
    "\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# for timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    #print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "           test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda :1.0)  # to selectively-print only best errors achieved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2016-10-02 20:06:20.106386\n",
      " 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 621.8\n",
      "completed pass 1 at alpha 0.025000\n",
      " 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 575.2\n",
      "completed pass 2 at alpha 0.023800\n",
      " 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 559.3\n",
      "completed pass 3 at alpha 0.022600\n",
      " 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 555.7\n",
      "completed pass 4 at alpha 0.021400\n",
      " 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 546.4\n",
      "completed pass 5 at alpha 0.020200\n",
      " 6 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 539.3\n",
      "completed pass 6 at alpha 0.019000\n",
      " 7 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 525.9\n",
      "completed pass 7 at alpha 0.017800\n",
      " 8 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 522.7\n",
      "completed pass 8 at alpha 0.016600\n",
      " 9 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 535.2\n",
      "completed pass 9 at alpha 0.015400\n",
      " 10 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 500.1\n",
      "completed pass 10 at alpha 0.014200\n",
      " 11 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 487.5\n",
      "completed pass 11 at alpha 0.013000\n",
      " 12 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 489.6\n",
      "completed pass 12 at alpha 0.011800\n",
      " 13 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 491.1\n",
      "completed pass 13 at alpha 0.010600\n",
      " 14 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 485.0\n",
      "completed pass 14 at alpha 0.009400\n",
      " 15 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 481.7\n",
      "completed pass 15 at alpha 0.008200\n",
      " 16 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 487.2\n",
      "completed pass 16 at alpha 0.007000\n",
      " 17 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 477.0\n",
      "completed pass 17 at alpha 0.005800\n",
      " 18 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 467.5\n",
      "completed pass 18 at alpha 0.004600\n",
      " 19 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 468.4\n",
      "completed pass 19 at alpha 0.003400\n",
      " 20 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,t4)_inferred 470.9\n",
      "completed pass 20 at alpha 0.002200\n",
      "END 2016-10-02 22:57:54.837282\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "\n",
    "train_docs = alldocs[0:7500]\n",
    "test_docs=alldocs[7501:9000]\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(alldocs)  # shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(alldocs)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "#         # evaluate\n",
    "#         eval_duration = ''\n",
    "#         with elapsed_timer() as eval_elapsed:\n",
    "#             err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "#         eval_duration = '%.1f' % eval_elapsed()\n",
    "#         best_indicator = ' '\n",
    "#         if err <= best_error[name]:\n",
    "#             best_error[name] = err\n",
    "#             best_indicator = '*' \n",
    "#         print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "#         if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "#             eval_duration = ''\n",
    "#             with elapsed_timer() as eval_elapsed:\n",
    "#                 infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "#             eval_duration = '%.1f' % eval_elapsed()\n",
    "#             best_indicator = ' '\n",
    "#             if infer_err < best_error[name + '_inferred']:\n",
    "#                 best_error[name + '_inferred'] = infer_err\n",
    "#                 best_indicator = '*'\n",
    "        print(\" %i passes : %s %s\" % ( epoch + 1, name + '_inferred', duration))\n",
    "\n",
    "    print('completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 244173...\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t4):\n",
      " [(29743, 0.563130795955658), (41351, 0.5335853695869446), (34357, 0.43929237127304077)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET (178629): «tycho unresolved dependencies i'm working on project lately which goal it is to build an eclipse plugin for palladio my project is built out of several seperate parts and managed to eliminate all errors but the last package still doesn't want to go through i'm using maven \"clean works perfectly the error appears when try maven install with tycho and some parts of palladio i've tried to call mvn clean via the console as well as in eclipse the error stays the same suspect there is something missing/wrong in the pom.xml of the last part of the plugin as all the other parts work and maven can clean/install them without an error message i've been working on this for the past week but cant seem to solve it any help is appreciated the following is the pom (i changed user name project name so don't mind that and here is the error i'm getting feature.xml of myproject.featurepatch feature xml of simucom »\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/c,d100,n5,w5,mc2,t4):\n",
      "\n",
      "MOST (101576, 0.4662477970123291): «two objects of different class to jsp and displaying information i'm learning how to use jdbc with jsp and i've encountered problem that i've been trying to figure out to no avail and was hoping someone could provide some insight my problem is the starlist object ends up being empty for some reason while the genrelist is fine am able to see based on the user input the type of movies from the selected genre but the associated stars for each movie do not show suspect it may be because of my function to search for stars from the list would try to debug and find the error but the means by which established tomcat and eclipse and my lack of knowledge with this type of programming make it very difficult any help is appreciated thank you <a href http /i.stack.imgur.com/mqxyn.jpg rel nofollow output of jsp »\n",
      "\n",
      "MEDIAN (242096, 0.011798292398452759): «image in spring mvc am using spring and hibernate to upload and retrieve image to and from database have converted multipart image into byte array and stored in database my query is how to retrieve that image from database and display the byte array in jsp without storing it in local system »\n",
      "\n",
      "LEAST (54233, -0.4639345109462738): «does not initialize many-to-one relation after merge have the following class with the following uni-directional mapping to company create new entry,set cardnumber=12 companyid= and and save it with session.merge the company with id= exists can load it with session.load the company does not have any hibernate-relation back to entry the returned persisted entry has getcompanyid but getcompany null why isn't the many-to-one relation initialized by hibernate in the newly persisted object if switch so that the companyid attribute is insert/update=false can set an existing company object instead but then the getcompanyid null instead of the id of the company in the database the companyid column is correctly saved am using hibernate 3.6.10 »\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
